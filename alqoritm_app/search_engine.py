# alqoritm_app/search_engine.py

import pandas as pd
import re
from fuzzywuzzy import fuzz
import advertools as adv
from datetime import datetime
import os

# Master vÉ™ test sÃ¼tunlarÄ±
MASTER_SERVICE_COLUMN = 'MallarÄ±n (iÅŸlÉ™rin vÉ™ xidmÉ™tlÉ™rin) adÄ±'
TEST_QUERY_COLUMN = 'MallarÄ±n (iÅŸlÉ™rin vÉ™ xidmÉ™tlÉ™rin) adÄ±'

# Sinonim bazasÄ±
SYNONYM_KNOWLEDGE_BASE = {
    "boya": "boya", "rÉ™nglÉ™nmÉ™": "boya", "kraska": "boya", "aÄŸardÄ±lmasÄ±": "boya", "tÉ™mir": "tÉ™mir", "remont": "tÉ™mir", "tÉ™miri": "tÉ™mir", "tÉ™mirin": "tÉ™mir", "tÉ™mÄ±rÄ±": "tÉ™mir", "tÉ™mÄ±rÄ±n": "tÉ™mir", "bÉ™rpa": "tÉ™mir", "dÉ™yiÅŸdirilmÉ™si": "tÉ™mir",
    "alÃ§ipan": "alÃ§ipan", "alcipan": "alÃ§ipan", "alcÄ±panl": "alÃ§ipan", "gipsokarton": "alÃ§ipan", "alÃ§Ä±panla": "alÃ§ipan",
    "divar": "divar", "divarlar": "divar", "divarÄ±n": "divar", "divarlarÄ±n": "divar", "dÄ±var": "divar", "arakÉ™smÉ™": "divar", "divarlarÄ±nÄ±n": "divar",
    "kafel": "kafel", "metlax": "kafel", "kafe": "kafel", "kafelinin": "kafel", "metlaxÄ±n": "kafel",
    "dÃ¶ÅŸÉ™mÉ™": "dÃ¶ÅŸÉ™mÉ™", "dÃ¶ÅŸÉ™nmÉ™": "dÃ¶ÅŸÉ™mÉ™", "dÃ¶ÅŸÉ™mÉ™sinin": "dÃ¶ÅŸÉ™mÉ™", "laminat": "dÃ¶ÅŸÉ™mÉ™", "parket": "dÃ¶ÅŸÉ™mÉ™", "pol": "dÃ¶ÅŸÉ™mÉ™", "laminatdan": "dÃ¶ÅŸÉ™mÉ™",
    "quraÅŸdÄ±rma": "quraÅŸdÄ±rma", "quraÅŸdÄ±rÄ±lmasÄ±": "quraÅŸdÄ±rma", "montaj": "quraÅŸdÄ±rma", "montaji": "quraÅŸdÄ±rma", "qurulmasÄ±": "quraÅŸdÄ±rma",
    "Ã§É™kilmÉ™": "quraÅŸdÄ±rma", "Ã§É™kilmÉ™si": "quraÅŸdÄ±rma", "vurulma": "quraÅŸdÄ±rma", "yÄ±ÄŸÄ±lma": "quraÅŸdÄ±rma",
    "yÄ±ÄŸÄ±l": "quraÅŸdÄ±rma", "qurul": "quraÅŸdÄ±rma", "qoyulmasÄ±": "quraÅŸdÄ±rma",
    "sÃ¶kÃ¼lmÉ™": "sÃ¶kÃ¼lmÉ™", "sÃ¶kÃ¼lmÉ™si": "sÃ¶kÃ¼lmÉ™", "sÃ¶kÃ¼ntÃ¼": "sÃ¶kÃ¼lmÉ™", "sÃ¶kÃ¼ntÃ¼sÃ¼": "sÃ¶kÃ¼lmÉ™",
    "tÉ™mizlÉ™nmÉ™": "tÉ™mizlÉ™nmÉ™", "tÉ™mizlÉ™nmÉ™si": "tÉ™mizlÉ™nmÉ™",
    "daÅŸÄ±nmasÄ±": "daÅŸÄ±nma", "daÅŸÄ±nma": "daÅŸÄ±nma",
    "izolyasiya": "izolyasiya", "hidroizolyasiya": "izolyasiya", "izolyasiyalÄ±": "izolyasiya",
    "suvaq": "suvaq", "suvaÄŸÄ±": "suvaq", "ÅŸpatlyovka": "suvaq", "sÉ™thinin": "suvaq", "sÉ™thin": "suvaq",
    "beton": "beton", "betonlanmasÄ±": "beton",
    "tavan": "tavan", "tavanÄ±n": "tavan", "tavanlarÄ±n": "tavan", "asma tavan": "tavan",
    "boru": "boru", "borularÄ±n": "boru",
    "sistem": "sistem", "sisteminin": "sistem",
    "drenaj": "drenaj", "havalandÄ±rma": "havalandÄ±rma",
    "metal": "metal", "dÉ™mir": "metal",
    "Ã§É™n": "Ã§É™n", "Ã§É™ni": "Ã§É™n",
    "pÉ™ncÉ™rÉ™": "pÉ™ncÉ™rÉ™", "pÉ™ncÉ™rÉ™lÉ™rin": "pÉ™ncÉ™rÉ™", "pvc": "pÉ™ncÉ™rÉ™", "plastik": "pÉ™ncÉ™rÉ™",
    "qapÄ±": "qapÄ±", "qapÄ±larÄ±n": "qapÄ±", "mdf qapÄ±larÄ±n": "qapÄ±",
    "taxta": "taxta", "plitÉ™": "plitÉ™", "ÅŸÃ¼ÅŸÉ™": "ÅŸÃ¼ÅŸÉ™",
    "ÅŸifer": "ÅŸifer", "polikarbonat": "polikarbonat",
    "elektrik": "elektrik", "santexnik": "santexnik", "santexnika": "santexnik",
    "aboy": "aboy", "divar kaÄŸÄ±zÄ±": "aboy", "divar kagizi": "aboy", "paduqa": "paduqa",
    "iÅŸlÉ™ri": "", "iÅŸi": "", "qiymÉ™ti": "", "neÃ§É™yÉ™dir": "", "axtarÄ±ram": "", "lazÄ±mdÄ±r": "", "olunmasÄ±": "",
    "otaÄŸÄ±n": "otaq", "evin": "ev", "mÉ™nzildÉ™": "ev", "hamam": "hamam", "mÉ™tbÉ™x": "mÉ™tbÉ™x",
    "isti pol": "isti pol", "ustasi": "usta", "profilsiz": "profilsiz",
}

RAW_CRITICAL_FEATURES = {"profilsiz", "izolyasiyalÄ±", "sÃ¶kÃ¼lmÉ™", "quraÅŸdÄ±rma"}
NORMALIZED_CRITICAL_FEATURES = {word.lower().replace('g', 'ÄŸ').replace('i', 'Ä±') for word in RAW_CRITICAL_FEATURES}
azerbaijani_stopwords = adv.stopwords['azerbaijani']

def preprocess_and_standardize(text, knowledge_base):
    if not isinstance(text, str): return ""
    for phrase, canonical in knowledge_base.items():
        if len(phrase.split()) > 1: text = text.replace(phrase, canonical)
    text = text.lower().replace('g', 'ÄŸ').replace('i', 'Ä±')
    text = re.sub(r'[^\w\s]', '', text)
    tokens = text.split()
    tokens = [word for word in tokens if word not in azerbaijani_stopwords]
    suffixes = [
        'lanmasÄ±', 'lÉ™nmÉ™si', 'lanma', 'lÉ™nmÉ™', 'nmasÄ±', 'nmÉ™si', 'masÄ±', 'mÉ™si', 'larÄ±n', 'lÉ™rin', 'larÄ±', 'lÉ™ri', 'lar', 'lÉ™r', 'dan', 'dÉ™n', 'Ä±n', 'in', 'un', 'Ã¼n',
        'da', 'dÉ™', 'a', 'É™', 'Ä±', 'u', 'Ã¼', 'ma', 'mÉ™', 'sÄ±', 'si', 'la', 'lÉ™'
    ]
    suffixes.sort(key=len, reverse=True)
    final_tokens = []
    for token in tokens:
        if token in knowledge_base:
            final_tokens.append(knowledge_base[token])
            continue
        stripped_token = token
        for suffix in suffixes:
            if stripped_token.endswith(suffix):
                stripped_token = stripped_token[:-len(suffix)]
                break
        if stripped_token in knowledge_base:
            final_tokens.append(knowledge_base[stripped_token])
        else:
            final_tokens.append(stripped_token)
    final_tokens = [token for token in final_tokens if token]
    return " ".join(final_tokens)

def find_top_matches(user_query, service_list, knowledge_base, top_n=5, min_score=65):
    canonical_query = preprocess_and_standardize(user_query, knowledge_base)
    query_tokens = set(canonical_query.split())
    all_matches = []
    for service in service_list:
        canonical_service = preprocess_and_standardize(service, knowledge_base)
        service_tokens = set(canonical_service.split())
        score = fuzz.token_set_ratio(canonical_query, canonical_service)
        for feature in NORMALIZED_CRITICAL_FEATURES:
            if (feature in query_tokens and feature not in service_tokens) or \
               (feature not in query_tokens and feature in service_tokens):
                score *= 0.70
                break
        if score >= min_score:
            all_matches.append({"match": service, "score": int(score)})
    sorted_matches = sorted(all_matches, key=lambda x: x['score'], reverse=True)
    return sorted_matches[:top_n]

def run_analysis(master_path, test_path, output_dir):
    print(f"ğŸ“¥ Master fayl yÃ¼klÉ™nir: {master_path}")
    master_db_df = pd.read_excel(master_path)
    MASTER_SERVICES_LIST = master_db_df[MASTER_SERVICE_COLUMN].dropna().tolist()
    print(f"âœ… {len(MASTER_SERVICES_LIST)} xidmÉ™t master-dÉ™n oxundu.")

    print(f"ğŸ“¥ Test fayl yÃ¼klÉ™nir: {test_path}")
    test_df = pd.read_excel(test_path)
    test_queries = test_df[TEST_QUERY_COLUMN].dropna().tolist()
    print(f"âœ… {len(test_queries)} test sorÄŸusu oxundu.")

    print("ğŸ” AnalizÉ™ baÅŸlanÄ±ldÄ±...\n")
    report_data = []

    for idx, query in enumerate(test_queries):
        if not query:
            continue
        print(f"â¡ï¸ ({idx + 1}/{len(test_queries)}) SorÄŸu: {query}")
        top_results = find_top_matches(query, MASTER_SERVICES_LIST, SYNONYM_KNOWLEDGE_BASE, top_n=5)
        print(f"   ğŸ”¸ Æn yaxÄ±n nÉ™ticÉ™(lÉ™r): {[r['match'] for r in top_results]}")

        row_data = {"Test Query": query}
        for j, result in enumerate(top_results):
            row_data[f"Match {j+1}"] = result['match']
            row_data[f"Score {j+1}"] = result['score']
        report_data.append(row_data)

    report_df = pd.DataFrame(report_data)

    timestamp = datetime.now().strftime("%Y-%m-%d_%H-%M-%S")
    output_filename = f"Customer_Analysis_Report_{timestamp}.xlsx"
    output_path = os.path.join(output_dir, output_filename)

    print(f"\nğŸ’¾ Excel faylÄ± saxlanÄ±lÄ±r: {output_path}")
    report_df.to_excel(output_path, index=False)
    print("âœ… Bitdi! Fayl hazÄ±rdÄ± vÉ™ brovserÉ™ gÃ¶ndÉ™rilÉ™cÉ™k.\n")

    return output_path
